{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# render ai gym environment\n",
    "import gym\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "env.observation_space Box(2,)\n",
      "env.observation_space.shape (2,)\n",
      "env.observation_space.low [-1.2  -0.07]\n",
      "env.observation_space.high [0.6  0.07]\n",
      "env.action_space Box(1,)\n",
      "env.action_space.shape (1,)\n",
      "env.action_space.low [-1.]\n",
      "env.action_space.high [1.]\n",
      "env.reward_range (-inf, inf)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "#print(dir(env))\n",
    "\n",
    "print('env.observation_space', env.observation_space)\n",
    "#print(dir(env.observation_space))\n",
    "print('env.observation_space.shape', env.observation_space.shape)\n",
    "print('env.observation_space.low', env.observation_space.low)\n",
    "print('env.observation_space.high', env.observation_space.high)\n",
    "\n",
    "print('env.action_space', env.action_space)\n",
    "#print(dir(env.action_space))\n",
    "print('env.action_space.shape', env.action_space.shape)\n",
    "print('env.action_space.low', env.action_space.low)\n",
    "print('env.action_space.high', env.action_space.high)\n",
    "\n",
    "\n",
    "\n",
    "print('env.reward_range', env.reward_range)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Policy(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, action_low, action_high, phi_size, noise_std):\n",
    "        super(Policy, self).__init__()\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.action_low = np.expand_dims(action_low, 0)\n",
    "        self.action_high = np.expand_dims(action_high, 0)\n",
    "        self.phi_size = phi_size\n",
    "        self.hidden_size = 64\n",
    "        \n",
    "        self.noise_std = noise_std\n",
    "\n",
    "        self.fc1 = nn.Linear(self.state_size, self.hidden_size, bias=True)\n",
    "        self.fc2 = nn.Linear(self.hidden_size, self.phi_size, bias=True)\n",
    "        \n",
    "        self.actor = nn.Linear(self.phi_size, self.action_size, bias=True)\n",
    "        self.critic = nn.Linear(self.phi_size+self.action_size, 1, bias=True)\n",
    "        \n",
    "    def _phi(self, state):\n",
    "        hidden = F.relu(self.fc1(state))\n",
    "        phi = F.relu(self.fc2(hidden))\n",
    "        return phi\n",
    "    \n",
    "    def _action(self, state):\n",
    "        phi = self._phi(state)\n",
    "        \n",
    "        _actor_mean = F.sigmoid(self.actor(phi)) * (self.action_high - self.action_low) + self.action_low\n",
    "        _actor_noise = torch.randn(_actor_mean.size()) * self.noise_std\n",
    "        _actor_out = torch.clamp(_actor_mean + _actor_noise, self.action_low, self.action_high)\n",
    "        \n",
    "        return _actor_out, _actor_mean\n",
    "    \n",
    "    def _critic(self, state, action):\n",
    "        _critic_out = self.critic(torch.cat([phi, action], dim=1))\n",
    "        \n",
    "        return _critic_out\n",
    "       \n",
    "    def forward(self, state, action):\n",
    "    \n",
    "        hidden = F.relu(self.fc1(state))\n",
    "        phi = F.relu(self.fc2(hidden))\n",
    "        \n",
    "        _actor_mean = F.sigmoid(self.actor(phi)) * (self.action_high - self.action_low) + self.action_low\n",
    "        _actor_noise = torch.randn(_actor_mean.size()) * self.noise_std\n",
    "        _actor_out = torch.clamp(_actor_mean + _actor_noise, self.action_low, self.action_high)\n",
    "        \n",
    "        _critic_out = self.critic(phi)        \n",
    "\n",
    "        return x\n",
    "    \n",
    "'''\n",
    "    def forward(self, obs):\n",
    "        phi = self.feature(obs)\n",
    "        action = self.actor(phi)\n",
    "        return action\n",
    "\n",
    "    def feature(self, obs):\n",
    "        obs = tensor(obs)\n",
    "        return self.network.phi_body(obs)\n",
    "\n",
    "    def actor(self, phi):\n",
    "        return F.tanh(self.network.fc_action(self.network.actor_body(phi)))\n",
    "\n",
    "    def critic(self, phi, a):\n",
    "        return self.network.fc_critic(self.network.critic_body(phi, a))    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop max iterations\n",
    "episode = 500\n",
    "\n",
    "# widget bar to display progress\n",
    "!pip install progressbar\n",
    "import progressbar as pb\n",
    "widget = ['training loop: ', pb.Percentage(), ' ', \n",
    "          pb.Bar(), ' ', pb.ETA() ]\n",
    "timer = pb.ProgressBar(widgets=widget, maxval=episode).start()\n",
    "\n",
    "# keep track of progress\n",
    "mean_rewards = []\n",
    "\n",
    "for e in range(episode):\n",
    "\n",
    "    # collect trajectories\n",
    "    old_probs, states, actions, rewards = \\\n",
    "        pong_utils.collect_trajectories(envs, policy, tmax=tmax)\n",
    "        \n",
    "    total_rewards = np.sum(rewards, axis=0)\n",
    "\n",
    "\n",
    "    # gradient ascent step\n",
    "    for _ in range(SGD_epoch):\n",
    "        \n",
    "        # uncomment to utilize your own clipped function!\n",
    "        L = -clipped_surrogate(policy, old_probs, states, actions, rewards, epsilon=epsilon, beta=beta)\n",
    "\n",
    "        # L = -pong_utils.clipped_surrogate(policy, old_probs, states, actions, rewards,\n",
    "        #                                    epsilon=epsilon, beta=beta)\n",
    "        optimizer.zero_grad()\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "        del L\n",
    "    \n",
    "    # the clipping parameter reduces as time goes on\n",
    "    epsilon*=.999\n",
    "    \n",
    "    # the regulation term also reduces\n",
    "    # this reduces exploration in later runs\n",
    "    beta*=.995\n",
    "    \n",
    "    # get the average reward of the parallel environments\n",
    "    mean_rewards.append(np.mean(total_rewards))\n",
    "    \n",
    "    # display some progress every 20 iterations\n",
    "    if (e+1)%20 ==0 :\n",
    "        print(\"Episode: {0:d}, score: {1:f}\".format(e+1,np.mean(total_rewards)))\n",
    "        print(total_rewards)\n",
    "        \n",
    "    # update progress widget bar\n",
    "    timer.update(e+1)\n",
    "    \n",
    "timer.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parallelEnv import parallelEnv\n",
    "import numpy as np\n",
    "# keep track of how long training takes\n",
    "# WARNING: running through all 800 episodes will take 30-45 minutes\n",
    "\n",
    "# training loop max iterations\n",
    "episode = 500\n",
    "\n",
    "# widget bar to display progress\n",
    "!pip install progressbar\n",
    "import progressbar as pb\n",
    "widget = ['training loop: ', pb.Percentage(), ' ', \n",
    "          pb.Bar(), ' ', pb.ETA() ]\n",
    "timer = pb.ProgressBar(widgets=widget, maxval=episode).start()\n",
    "\n",
    "\n",
    "envs = parallelEnv('PongDeterministic-v4', n=8, seed=1234)\n",
    "\n",
    "discount_rate = .99\n",
    "epsilon = 0.1\n",
    "beta = .01\n",
    "tmax = 320\n",
    "SGD_epoch = 4\n",
    "\n",
    "# keep track of progress\n",
    "mean_rewards = []\n",
    "\n",
    "for e in range(episode):\n",
    "\n",
    "    # collect trajectories\n",
    "    old_probs, states, actions, rewards = \\\n",
    "        pong_utils.collect_trajectories(envs, policy, tmax=tmax)\n",
    "        \n",
    "    total_rewards = np.sum(rewards, axis=0)\n",
    "\n",
    "\n",
    "    # gradient ascent step\n",
    "    for _ in range(SGD_epoch):\n",
    "        \n",
    "        # uncomment to utilize your own clipped function!\n",
    "        L = -clipped_surrogate(policy, old_probs, states, actions, rewards, epsilon=epsilon, beta=beta)\n",
    "\n",
    "        # L = -pong_utils.clipped_surrogate(policy, old_probs, states, actions, rewards,\n",
    "        #                                    epsilon=epsilon, beta=beta)\n",
    "        optimizer.zero_grad()\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "        del L\n",
    "    \n",
    "    # the clipping parameter reduces as time goes on\n",
    "    epsilon*=.999\n",
    "    \n",
    "    # the regulation term also reduces\n",
    "    # this reduces exploration in later runs\n",
    "    beta*=.995\n",
    "    \n",
    "    # get the average reward of the parallel environments\n",
    "    mean_rewards.append(np.mean(total_rewards))\n",
    "    \n",
    "    # display some progress every 20 iterations\n",
    "    if (e+1)%20 ==0 :\n",
    "        print(\"Episode: {0:d}, score: {1:f}\".format(e+1,np.mean(total_rewards)))\n",
    "        print(total_rewards)\n",
    "        \n",
    "    # update progress widget bar\n",
    "    timer.update(e+1)\n",
    "    \n",
    "timer.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
