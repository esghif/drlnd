{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# render ai gym environment\n",
    "import gym\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "env.observation_space Box(2,)\n",
      "env.observation_space.shape (2,)\n",
      "env.observation_space.low [-1.2  -0.07]\n",
      "env.observation_space.high [0.6  0.07]\n",
      "env.action_space Box(1,)\n",
      "env.action_space.shape (1,)\n",
      "env.action_space.low [-1.]\n",
      "env.action_space.high [1.]\n",
      "env.reward_range (-inf, inf)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "#print(dir(env))\n",
    "\n",
    "print('env.observation_space', env.observation_space)\n",
    "#print(dir(env.observation_space))\n",
    "print('env.observation_space.shape', env.observation_space.shape)\n",
    "print('env.observation_space.low', env.observation_space.low)\n",
    "print('env.observation_space.high', env.observation_space.high)\n",
    "\n",
    "print('env.action_space', env.action_space)\n",
    "#print(dir(env.action_space))\n",
    "print('env.action_space.shape', env.action_space.shape)\n",
    "print('env.action_space.low', env.action_space.low)\n",
    "print('env.action_space.high', env.action_space.high)\n",
    "\n",
    "print('env.reward_range', env.reward_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DDPGConfig = {\n",
    "    'net' : {\n",
    "            'state_size': 2,\n",
    "            'action_size': 1,\n",
    "            'phi_size': 16,\n",
    "            'seed': 0,\n",
    "        },\n",
    "    'opt': {\n",
    "            'actor_lr': 1e-3,\n",
    "            'critic_lr': 1e-3,        \n",
    "        },\n",
    "    'replay_buffer': {\n",
    "        'size': int(1e5),\n",
    "    },\n",
    "    'env': {\n",
    "        'action_low': [-1],\n",
    "        'action_high' [1],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    \\n    def _action(self, state):\\n        phi = self._phi(state)\\n        \\n        _actor_mean = F.sigmoid(self.actor(phi)) * (self.action_high - self.action_low) + self.action_low\\n        _actor_noise = torch.randn(_actor_mean.size()) * self.noise_std\\n        _actor_out = torch.clamp(_actor_mean + _actor_noise, self.action_low, self.action_high)\\n        \\n        return _actor_out, _actor_mean\\n    \\n    def _critic(self, state, action):\\n        _critic_out = self.critic(torch.cat([phi, action], dim=1))\\n        \\n        return _critic_out\\n       \\n    def forward(self, state, action):\\n    \\n        hidden = F.relu(self.fc1(state))\\n        phi = F.relu(self.fc2(hidden))\\n        \\n        _actor_mean = F.sigmoid(self.actor(phi)) * (self.action_high - self.action_low) + self.action_low\\n        _actor_noise = torch.randn(_actor_mean.size()) * self.noise_std\\n        _actor_out = torch.clamp(_actor_mean + _actor_noise, self.action_low, self.action_high)\\n        \\n        _critic_out = self.critic(phi)        \\n\\n        return x\\n    \\n\\n    def forward(self, obs):\\n        phi = self.feature(obs)\\n        action = self.actor(phi)\\n        return action\\n\\n    def feature(self, obs):\\n        obs = tensor(obs)\\n        return self.network.phi_body(obs)\\n\\n    def actor(self, phi):\\n        return F.tanh(self.network.fc_action(self.network.actor_body(phi)))\\n\\n    def critic(self, phi, a):\\n        return self.network.fc_critic(self.network.critic_body(phi, a))    '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ActorCriticPolicy(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, phi_size, seed):\n",
    "        super(Policy, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        #self.action_low = np.expand_dims(action_low, 0)\n",
    "        #self.action_high = np.expand_dims(action_high, 0)\n",
    "        self.phi_size = phi_size\n",
    "        self.hidden_size = 64\n",
    "        \n",
    "        self.noise_std = noise_std\n",
    "\n",
    "        self.fc1 = nn.Linear(self.state_size, self.hidden_size, bias=True)\n",
    "        self.fc2 = nn.Linear(self.hidden_size, self.phi_size, bias=True)\n",
    "        \n",
    "        self.fc_actor = nn.Linear(self.phi_size, self.action_size, bias=True)\n",
    "        self.fc_critic = nn.Linear(self.phi_size+self.action_size, 1, bias=True)\n",
    "        \n",
    "    def feature(self, state):\n",
    "        hidden = F.relu(self.fc1(state))\n",
    "        phi = F.relu(self.fc2(hidden))\n",
    "        return phi\n",
    "    \n",
    "    def actor(self, phi):\n",
    "        action = F.tanh(self.fc_actor(phi))\n",
    "        return action\n",
    "    \n",
    "    def critic(self, phi, a):\n",
    "        Q = self.fc_critic(torch.cat([phi,a], dim=1))\n",
    "        return Q\n",
    "    \n",
    "    def forward(self, state):\n",
    "        phi = self.feature(state)\n",
    "        action = self.actor(phi)\n",
    "        return action \n",
    "#, action_low, action_high\n",
    "    \n",
    "class DDPGAgent():\n",
    "    \n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        self.seed = np.random.seed(cfg['net']['seed'])\n",
    "        \n",
    "        self.qnetwork_local = ActorCriticPolicy(cfg['net']).to(device)\n",
    "        self.qnetwork_target = ActorCriticPolicy(cfg['net']).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=cfg['opt']['actor_lr'])\n",
    "        self.critic_optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=cfg['opt']['actor_lr'])\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(cfg['replay_buffer']['size'])\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "        \n",
    "    def act(self, state, std=0.):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            actions_local = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "        \n",
    "        actions_local = actions_local.cpu().data.numpy()\n",
    "        noise = np.random.randn(*actions_local.shape) * std\n",
    "        actions_local += noise\n",
    "        actions_local = np.clip(actions_local, cfg['env']['action_low'], cfg['env']['action_high'])\n",
    "\n",
    "        return actions_local\n",
    "        \n",
    "    '''\n",
    "    def step(self, state, action, reward, next_state, done):        \n",
    "        idx = self.memory._next_idx\n",
    "        \n",
    "        # Add sample buffer to replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        states = np.expand_dims(state, axis=0)\n",
    "        actions = np.expand_dims(np.expand_dims(action, axis=0), axis=-1)\n",
    "        rewards = np.expand_dims(reward, axis=0)        \n",
    "        next_states = np.expand_dims(next_state, axis=0)        \n",
    "        dones = np.expand_dims(done, axis=0)\n",
    "        \n",
    "        delta = self.compute_delta(states, actions, rewards, next_states, dones, GAMMA)\n",
    "        priorities = np.abs(np.squeeze(delta.cpu().detach().numpy(), axis=0)) + PRIORITIZED_REPLAY_EPS\n",
    "        self.memory.update_priorities([idx], priorities)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            \n",
    "          \n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.memory.sample(BATCH_SIZE, self.beta(done))\n",
    "                self.memory.alpha = self.alpha(done)\n",
    "                self.learn(experiences, GAMMA)\n",
    "                               \n",
    "                \n",
    "\n",
    "    \n",
    "    def compute_delta(self, states, actions, rewards, next_states, dones, gamma):\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        actions = torch.from_numpy(actions).long().to(device)\n",
    "        rewards = torch.from_numpy(rewards).float().to(device)\n",
    "        next_states = torch.from_numpy(next_states).float().to(device)\n",
    "        dones = torch.from_numpy(dones.astype(np.uint8)).float().to(device)\n",
    "        \n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        # Double DQN\n",
    "        actions_next_state = self.qnetwork_local(states).detach().argmax(1).unsqueeze(1)\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().gather(1, actions_next_state)\n",
    "        # DQN\n",
    "        # Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        # Compute Q targets for current states\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        # Get expected Q values from local model\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "        \n",
    "        # Compute loss\n",
    "        delta = Q_targets - Q_expected\n",
    "        \n",
    "        return delta\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        states, actions, rewards, next_states, dones, weights, indices = experiences\n",
    "        \n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        actions = torch.from_numpy(np.expand_dims(actions, axis=-1)).long().to(device)\n",
    "        rewards = torch.from_numpy(np.expand_dims(rewards, axis=-1)).float().to(device)\n",
    "        next_states = torch.from_numpy(next_states).float().to(device)\n",
    "        dones = torch.from_numpy(np.expand_dims(dones.astype(np.uint8), axis=-1)).float().to(device)        \n",
    "        weights = torch.from_numpy(weights).float().to(device)\n",
    "\n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        # Double DQN\n",
    "        actions_next_state = self.qnetwork_local(states).detach().argmax(1).unsqueeze(1)\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().gather(1, actions_next_state)\n",
    "        # DQN\n",
    "        # Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        # Compute Q targets for current states\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        # Get expected Q values from local model\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "        \n",
    "        # Compute loss\n",
    "        delta = Q_targets - Q_expected\n",
    "        squared_difference = delta ** 2\n",
    "        weighted_squared_difference = weights * squared_difference\n",
    "        loss = weighted_squared_difference.mean()\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU) \n",
    "\n",
    "        # ------------------- update priorities ------------------- #        \n",
    "        new_priorities = np.abs(np.squeeze(delta.cpu().detach().numpy())) + PRIORITIZED_REPLAY_EPS\n",
    "        self.memory.update_priorities(indices, new_priorities)\n",
    "        \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "    '''\n",
    "            \n",
    "def ddpg(agent, n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]            # get the current state\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)                 # select an action\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished       \n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=13.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "scores = ddpg(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995)\n",
    "end = time.time()\n",
    "print('time = {:.2f} s'.format(end-start))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]            # get the current state\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)                 # select an action\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished       \n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=13.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "scores = dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995)\n",
    "end = time.time()\n",
    "print('time = {:.2f} s'.format(end-start))\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting progressbar\n",
      "\u001b[33m  Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f56f2248550>: Failed to establish a new connection: [Errno 111] Connection refused',))': /simple/progressbar/\u001b[0m\n",
      "\u001b[33m  Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f56f33a3668>: Failed to establish a new connection: [Errno 111] Connection refused',))': /simple/progressbar/\u001b[0m\n",
      "\u001b[33m  Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f56f33a3128>: Failed to establish a new connection: [Errno 111] Connection refused',))': /simple/progressbar/\u001b[0m\n",
      "\u001b[33m  Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f56f33a34a8>: Failed to establish a new connection: [Errno 111] Connection refused',))': /simple/progressbar/\u001b[0m\n",
      "\u001b[33m  Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f56f5a45438>: Failed to establish a new connection: [Errno 111] Connection refused',))': /simple/progressbar/\u001b[0m\n",
      "\u001b[31m  Could not find a version that satisfies the requirement progressbar (from versions: )\u001b[0m\n",
      "\u001b[31mNo matching distribution found for progressbar\u001b[0m\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'progressbar'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-075cad998b19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# widget bar to display progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install progressbar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mprogressbar\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m widget = ['training loop: ', pb.Percentage(), ' ', \n\u001b[1;32m      8\u001b[0m           pb.Bar(), ' ', pb.ETA() ]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'progressbar'"
     ]
    }
   ],
   "source": [
    "# training loop max iterations\n",
    "episode = 500\n",
    "\n",
    "# widget bar to display progress\n",
    "!pip install progressbar\n",
    "import progressbar as pb\n",
    "widget = ['training loop: ', pb.Percentage(), ' ', \n",
    "          pb.Bar(), ' ', pb.ETA() ]\n",
    "timer = pb.ProgressBar(widgets=widget, maxval=episode).start()\n",
    "\n",
    "# keep track of progress\n",
    "mean_rewards = []\n",
    "\n",
    "for e in range(episode):\n",
    "\n",
    "    # collect trajectories\n",
    "    old_probs, states, actions, rewards = \\\n",
    "        pong_utils.collect_trajectories(envs, policy, tmax=tmax)\n",
    "        \n",
    "    total_rewards = np.sum(rewards, axis=0)\n",
    "\n",
    "\n",
    "    # gradient ascent step\n",
    "    for _ in range(SGD_epoch):\n",
    "        \n",
    "        # uncomment to utilize your own clipped function!\n",
    "        L = -clipped_surrogate(policy, old_probs, states, actions, rewards, epsilon=epsilon, beta=beta)\n",
    "\n",
    "        # L = -pong_utils.clipped_surrogate(policy, old_probs, states, actions, rewards,\n",
    "        #                                    epsilon=epsilon, beta=beta)\n",
    "        optimizer.zero_grad()\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "        del L\n",
    "    \n",
    "    # the clipping parameter reduces as time goes on\n",
    "    epsilon*=.999\n",
    "    \n",
    "    # the regulation term also reduces\n",
    "    # this reduces exploration in later runs\n",
    "    beta*=.995\n",
    "    \n",
    "    # get the average reward of the parallel environments\n",
    "    mean_rewards.append(np.mean(total_rewards))\n",
    "    \n",
    "    # display some progress every 20 iterations\n",
    "    if (e+1)%20 ==0 :\n",
    "        print(\"Episode: {0:d}, score: {1:f}\".format(e+1,np.mean(total_rewards)))\n",
    "        print(total_rewards)\n",
    "        \n",
    "    # update progress widget bar\n",
    "    timer.update(e+1)\n",
    "    \n",
    "timer.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parallelEnv import parallelEnv\n",
    "import numpy as np\n",
    "# keep track of how long training takes\n",
    "# WARNING: running through all 800 episodes will take 30-45 minutes\n",
    "\n",
    "# training loop max iterations\n",
    "episode = 500\n",
    "\n",
    "# widget bar to display progress\n",
    "!pip install progressbar\n",
    "import progressbar as pb\n",
    "widget = ['training loop: ', pb.Percentage(), ' ', \n",
    "          pb.Bar(), ' ', pb.ETA() ]\n",
    "timer = pb.ProgressBar(widgets=widget, maxval=episode).start()\n",
    "\n",
    "\n",
    "envs = parallelEnv('PongDeterministic-v4', n=8, seed=1234)\n",
    "\n",
    "discount_rate = .99\n",
    "epsilon = 0.1\n",
    "beta = .01\n",
    "tmax = 320\n",
    "SGD_epoch = 4\n",
    "\n",
    "# keep track of progress\n",
    "mean_rewards = []\n",
    "\n",
    "for e in range(episode):\n",
    "\n",
    "    # collect trajectories\n",
    "    old_probs, states, actions, rewards = \\\n",
    "        pong_utils.collect_trajectories(envs, policy, tmax=tmax)\n",
    "        \n",
    "    total_rewards = np.sum(rewards, axis=0)\n",
    "\n",
    "\n",
    "    # gradient ascent step\n",
    "    for _ in range(SGD_epoch):\n",
    "        \n",
    "        # uncomment to utilize your own clipped function!\n",
    "        L = -clipped_surrogate(policy, old_probs, states, actions, rewards, epsilon=epsilon, beta=beta)\n",
    "\n",
    "        # L = -pong_utils.clipped_surrogate(policy, old_probs, states, actions, rewards,\n",
    "        #                                    epsilon=epsilon, beta=beta)\n",
    "        optimizer.zero_grad()\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "        del L\n",
    "    \n",
    "    # the clipping parameter reduces as time goes on\n",
    "    epsilon*=.999\n",
    "    \n",
    "    # the regulation term also reduces\n",
    "    # this reduces exploration in later runs\n",
    "    beta*=.995\n",
    "    \n",
    "    # get the average reward of the parallel environments\n",
    "    mean_rewards.append(np.mean(total_rewards))\n",
    "    \n",
    "    # display some progress every 20 iterations\n",
    "    if (e+1)%20 ==0 :\n",
    "        print(\"Episode: {0:d}, score: {1:f}\".format(e+1,np.mean(total_rewards)))\n",
    "        print(total_rewards)\n",
    "        \n",
    "    # update progress widget bar\n",
    "    timer.update(e+1)\n",
    "    \n",
    "timer.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            \n",
    "'''    \n",
    "    def _action(self, state):\n",
    "        phi = self._phi(state)\n",
    "        \n",
    "        _actor_mean = F.sigmoid(self.actor(phi)) * (self.action_high - self.action_low) + self.action_low\n",
    "        _actor_noise = torch.randn(_actor_mean.size()) * self.noise_std\n",
    "        _actor_out = torch.clamp(_actor_mean + _actor_noise, self.action_low, self.action_high)\n",
    "        \n",
    "        return _actor_out, _actor_mean\n",
    "    \n",
    "    def _critic(self, state, action):\n",
    "        _critic_out = self.critic(torch.cat([phi, action], dim=1))\n",
    "        \n",
    "        return _critic_out\n",
    "       \n",
    "    def forward(self, state, action):\n",
    "    \n",
    "        hidden = F.relu(self.fc1(state))\n",
    "        phi = F.relu(self.fc2(hidden))\n",
    "        \n",
    "        _actor_mean = F.sigmoid(self.actor(phi)) * (self.action_high - self.action_low) + self.action_low\n",
    "        _actor_noise = torch.randn(_actor_mean.size()) * self.noise_std\n",
    "        _actor_out = torch.clamp(_actor_mean + _actor_noise, self.action_low, self.action_high)\n",
    "        \n",
    "        _critic_out = self.critic(phi)        \n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "   '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
